{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81d700a6-ea19-43f1-a91c-694df2a791ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/n/home07/adamaraju/.local/lib/python3.8/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: libtorch_cuda_cu.so: cannot open shared object file: No such file or directory\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "from transformers import CLIPFeatureExtractor, CLIPTextModel, CLIPTokenizer\n",
    "\n",
    "from diffusers import (\n",
    "    AutoencoderKL,\n",
    "    T2IAdapter,\n",
    "    MultiAdapter,\n",
    "    StableDiffusionAdapterPipeline,\n",
    "    DDPMScheduler\n",
    ")\n",
    "\n",
    "import torch\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce4020bf-7bbf-418c-af38-1dd644e3c6cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.5, 511.5, 511.5, -0.5)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Generate some generic training data real quick\n",
    "\n",
    "\n",
    "import numpy as np \n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def generate_triangle_sdf(H,W):\n",
    "    \n",
    "    \n",
    "    EPS = 1e-12\n",
    "    x_pos, y_pos = np.meshgrid(np.linspace(0, (H-1), H),np.linspace(0, (W-1), W))\n",
    "\n",
    "    thresh = int(.2*W)\n",
    "    len_thresh = thresh*2\n",
    "    img_np = np.zeros((H, W), dtype=np.uint8)\n",
    "    valid2 = False\n",
    "    valid3 = False\n",
    "    while valid2 == False or valid3 == False:\n",
    "        x_1 = int(np.random.uniform(thresh,H-thresh))\n",
    "        y_1 = int(np.random.uniform(thresh,W-thresh))\n",
    "\n",
    "        #Compute a second point for the triangle\n",
    "        x_2 = int(np.random.uniform(thresh,H-thresh))\n",
    "        y_2 = int(np.random.uniform(thresh,W-thresh))\n",
    "        idx = 0 \n",
    "        while np.linalg.norm(np.array([x_1,y_1]) - np.array([x_2,y_2])) < len_thresh:\n",
    "            if idx < 50:\n",
    "                x_2 = int(np.random.uniform(thresh,H-thresh))\n",
    "                y_2 = int(np.random.uniform(thresh,W-thresh))\n",
    "                valid2 = True\n",
    "            else:\n",
    "                valid2 = False\n",
    "                break\n",
    "\n",
    "        x_3 = int(np.random.uniform(thresh,H-thresh))\n",
    "        y_3 = int(np.random.uniform(thresh,W-thresh))\n",
    "        idx = 0\n",
    "        while (np.linalg.norm(np.array([x_1,y_1]) - np.array([x_3,y_3])) < len_thresh) or (np.linalg.norm(np.array([x_2,y_2]) - np.array([x_3,y_3])) < len_thresh):\n",
    "            idx = idx +1\n",
    "            if idx < 50:\n",
    "                x_3 = int(np.random.uniform(thresh,H-thresh))\n",
    "                y_3 = int(np.random.uniform(thresh,W-thresh)) \n",
    "                valid3 = True\n",
    "            else:\n",
    "                valid3 = False\n",
    "                break\n",
    "    \n",
    "    pts_inp = np.array([[x_1, y_1], [x_2, y_2], [x_3, y_3]])\n",
    "    \n",
    "    oop_normal = [0,0,1]\n",
    "    \n",
    "    n = np.cross([x_1-x_2,y_1-y_2,0], [x_3-x_2,y_3-y_2])\n",
    "    \n",
    "    if np.dot(n,oop_normal) < 0:\n",
    "        pts_inp = pts_inp[[0,2,1],:]\n",
    "        x_2,y_2 = pts_inp[1,:]\n",
    "        x_3,y_3 = pts_inp[2,:]\n",
    "        \n",
    "    normals = []\n",
    "    ix_12 = np.linspace(x_1,x_2,500)\n",
    "    m_12 = (y_2-y_1)/(x_2-x_1 + EPS)\n",
    "    iy_12 = y_2 + m_12*(ix_12 - x_2)\n",
    "    out_12 =  np.array([[x, y] for (x, y) in zip(ix_12, iy_12)])\n",
    "    norm_12 = [np.arctan2(y_2-y_1, x_2-x_1)+np.pi/2] * len(ix_12)\n",
    "    normals.extend(norm_12)\n",
    "    \n",
    "    ix_23 = np.linspace(x_2,x_3,500)\n",
    "    m_23 = (y_3-y_2)/(x_3-x_2 + EPS)\n",
    "    iy_23 = y_3 + m_23*(ix_23 - x_3)\n",
    "    out_23 =  np.array([[x, y] for (x, y) in zip(ix_23, iy_23)])\n",
    "    norm_23 = [np.arctan2(y_3-y_2, x_3-x_2)+np.pi/2] * len(ix_23)\n",
    "    normals.extend(norm_23)\n",
    "    \n",
    "    ix_31 = np.linspace(x_3,x_1,500)\n",
    "    m_31 = (y_1-y_3)/(x_1-x_3 + EPS)\n",
    "    iy_31 = y_1 + m_31*(ix_31 - x_1)\n",
    "    out_31 =  np.array([[x, y] for (x, y) in zip(ix_31, iy_31)])\n",
    "    norm_31 = [np.arctan2(y_1-y_3, x_1-x_3)+np.pi/2] * len(ix_31)\n",
    "    normals.extend(norm_31)\n",
    "    \n",
    "    #Combine all 3 ouputs\n",
    "    out_pts = np.concatenate([out_12,out_23,out_31],axis=0)\n",
    "    out_pts = out_pts.reshape((1, -1, 2))\n",
    "\n",
    "    \n",
    "    pts = pts_inp.reshape((1,-1,2))\n",
    "    cv2.fillPoly(img_np, pts, color=255, lineType=cv2.LINE_AA)\n",
    "    img = img_np.reshape(img_np.shape[0],img_np.shape[1],1)\n",
    "    img = np.asarray(img/255.0,dtype=np.float32)\n",
    "    img = np.asarray(img>.5,dtype=np.float32)\n",
    "    normals = np.asarray(normals)\n",
    "    \n",
    "    return img, out_pts, normals\n",
    "\n",
    "H=W=512\n",
    "\n",
    "img,out_pts,normals = generate_triangle_sdf(H,W)\n",
    "img = 1- img\n",
    "first_img = (np.repeat(img,3,-1)* 255).astype(np.uint8)\n",
    "plt.imshow(first_img)\n",
    "\n",
    "mask_img,_,_ = generate_triangle_sdf(H,W)\n",
    "mask_img = mask_img[:,:,0].astype(int)\n",
    "\n",
    "\n",
    "masked_img = np.copy(img)\n",
    "masked_img[mask_img==1] = .5\n",
    "\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.imshow((np.repeat(masked_img,3,-1)* 255).astype(np.uint8),cmap='gray_r')\n",
    "\n",
    "plt.figure()\n",
    "plt.subplot(131)\n",
    "fg_layer = mask_img\n",
    "plt.imshow(fg_layer,cmap='gray',vmin=0,vmax=1)\n",
    "plt.xticks([]);plt.yticks([]);\n",
    "\n",
    "plt.subplot(132)\n",
    "mid_layer = (1-img).squeeze()\n",
    "plt.imshow(mid_layer,cmap='gray',vmin=0,vmax=1)\n",
    "plt.xticks([]);plt.yticks([]);\n",
    "\n",
    "plt.subplot(133)\n",
    "bg_layer = np.ones(mid_layer.shape)\n",
    "plt.imshow(bg_layer,cmap='gray',vmin=0,vmax=1)\n",
    "plt.xticks([]);plt.yticks([]);\n",
    "\n",
    "plt.figure()\n",
    "layer_img = np.stack([fg_layer,mid_layer,bg_layer],axis=-1)\n",
    "plt.imshow(layer_img)\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e7077a8-9412-47a9-8962-21976d5cdcf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "DATASET_SIZE=10\n",
    "H=W=512\n",
    "\n",
    "layer_binaries = []\n",
    "layer_imgs = []\n",
    "base_imgs = []\n",
    "\n",
    "def gray2rgb(img):\n",
    "    return (np.repeat(img,3,-1)* 255).astype(np.uint8)\n",
    "\n",
    "for itr in range(DATASET_SIZE):\n",
    "    img,_,_ = generate_triangle_sdf(H,W)\n",
    "    img = 1- img\n",
    "    \n",
    "    #Mask (foreground layer)\n",
    "    mask_img,_,_ = generate_triangle_sdf(H,W)\n",
    "    mask_img = mask_img[:,:,0].astype(int)\n",
    "\n",
    "    #Plain background layer\n",
    "    bg_layer = np.ones([H,W])\n",
    "    \n",
    "    #Compositing full image\n",
    "    masked_img = np.copy(img)\n",
    "    masked_img[mask_img==1] = .5\n",
    "    \n",
    "    #Compositing layer map\n",
    "    fg_layer = mask_img\n",
    "    mid_layer = (1-img).squeeze()\n",
    "    layer_bin = np.stack([fg_layer,mid_layer,bg_layer],axis=-1)\n",
    "\n",
    "    #add to lists\n",
    "    layer_binaries.append(layer_bin)\n",
    "    layer_imgs.append((gray2rgb(1-fg_layer[...,np.newaxis]*.5),\n",
    "                       gray2rgb(1-mid_layer[...,np.newaxis]),\n",
    "                       gray2rgb(bg_layer[...,np.newaxis])))\n",
    "    base_imgs.append(gray2rgb(masked_img))\n",
    "    \n",
    "    if itr%200 == 0:\n",
    "        print(itr)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68ff71d8-eb1c-43b5-b067-4d4c89f07033",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(512, 512, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_imgs[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "deb385a9-0ee3-4ca3-8034-8b205bc84656",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x14ddbccd20a0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQYAAAD8CAYAAACVSwr3AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAXaklEQVR4nO3dW2xUV57v8e8fE3Kd9EAwhAaSgNodOpk+uciio06r09OZPsnpGU3y0JGYaI54iJSXHKlHM9KIaKRzNA+R+pyH0TzlIZppDdJcIqSZUVA6pAkwARMb3zAGfAObgC/4SoEvsSmXq/7nwdvuSrbBZbuq9i7795Gs2rW8qupvYX5ee+219zZ3R0Qk27qoCxCR+FEwiEiIgkFEQhQMIhKiYBCREAWDiIQULBjM7FUz6zCzTjM7UKjPEZH8s0KsYzCzMuAS8DOgF6gH/szdW/P+YSKSd4UaMewFOt39irtPAx8CrxXos0Qkz9YX6H23Az1Zz3uBH9yp8+bNm/2JJ54oUCkiAtDY2Dji7uW59C1UMNgCbV/bZzGzt4G3AR577DEaGhoKVIqIAJjZtVz7FmpXohfYmfV8B3A9u4O7f+Dule5eWV6eU4iJSJEUKhjqgQoz22VmG4B9wOECfZaI5FlBdiXcfcbM/hfwW6AM+LW7txTis0Qk/wo1x4C7fwJ8Uqj3F5HC0cpHEQlRMIhIiIJBREIUDCISomAQkRAFg4iEKBhEJETBICIhCgYRCVEwiEiIgkFEQhQMIhKiYBCREAWDiIQoGEQkRMEgIiEKBhEJUTCISIiCQURCFAwiEqJgEJEQBYOIhCgYRCREwSAiIQoGEQlRMIhIiIJBREIUDCISomAQkRAFg4iEKBhEJETBICIhCgYRCVEwiEjIosFgZr82syEzu5jVtsnMPjOzy8HjxqzvvWtmnWbWYWavFKpwESmcXEYM/wS8+o22A8Bxd68AjgfPMbOngH3A08Fr3jezsrxVKyJFsWgwuPspIPGN5teAg8H2QeD1rPYP3T3p7l8CncDe/JQqIsWy3DmGre7eDxA8bgnatwM9Wf16g7YQM3vbzBrMrGF4eHiZZYhIIeR78tEWaPOFOrr7B+5e6e6V5eXleS5DRFZiucEwaGbbAILHoaC9F9iZ1W8HcH355YlIFJYbDIeB/cH2fuCjrPZ9Znavme0CKoC6lZUoIsW2frEOZvZvwE+AzWbWC/wf4FfAITN7C+gG3gBw9xYzOwS0AjPAO+6eLlDtIlIgiwaDu//ZHb718h36vwe8t5KiRCRaWvkoIiEKBhEJUTCISIiCQURCFAwiEqJgEJEQBYOIhCgYRCREwSAiIQoGEQlRMIhIiIJBREIUDCISomAQkRAFg4iEKBhEJETBICIhCgYRCVEwiEiIgkGKxt1JpVJRlyE5UDBIUbg7ra2tHD58mHRaFw6POwWDFMXk5CTHjx+nra2Nq1ev4r7gDcokJhQMUnCZTIampiYSiQQzMzMcO3aMr776Kuqy5C4UDFJQ7s7AwACnTp2ab+vv7+fChQsaNcSYgkEKanp6mo8//pjp6emvtdfX12vUEGMKBikYd6e9vZ3+/v7Q927evMn58+c1aogpBYMUTCKR4NixYwv+53d3qqqquH5dN0OPIwWDFMT09DQnTpxgfHz8jn2mpqaoqakhk8kUsTLJhYJBCuLKlSu0t7cv2q+jo2PBXQ2JloJB8m58fJzTp0/ntJAplUpRU1OjRU8xo2CQvMpkMtTX19Pb25vzay5dukRvb68mImNEwSB51dPTQ3V19ZJeMz09zSeffEIymSxQVbJUCgbJm7ndgpmZmSW/dnBwkNbWVo0aYmLRYDCznWb2X2bWZmYtZvbLoH2TmX1mZpeDx41Zr3nXzDrNrMPMXinkDyDx4O40NjbS0dGx7Peora3l9u3beaxKliuXEcMM8Ffu/j3gBeAdM3sKOAAcd/cK4HjwnOB7+4CngVeB982srBDFS3wMDAxw+vTpFf3FHxoaorm5WYcvY2DRYHD3fnc/G2yPA23AduA14GDQ7SDwerD9GvChuyfd/UugE9ib57olRtyd2tpaJiYmVvw+J0+eZGRkJE+VyXItaY7BzJ4AngNqga3u3g+z4QFsCbptB3qyXtYbtMkq1dbWRmtra17ea2pqirq6Os01RCznYDCzh4B/B/7C3cfu1nWBttC/spm9bWYNZtYwPDycaxkSM6Ojo1RVVYVOklqJlpYWjRoillMwmNk9zIbCv7j7fwTNg2a2Lfj+NmAoaO8Fdma9fAcQWhDv7h+4e6W7V5aXly+3folQJpPhzJkzeV+5ODU1RX19vUYNEcrlqIQB/wi0ufvfZX3rMLA/2N4PfJTVvs/M7jWzXUAFUJe/kiUO3J2hoSHOnz9fkPdvbm7m8uXLBXlvWVwuI4YXgf8J/NTMzgVfPwd+BfzMzC4DPwue4+4twCGgFfgUeMfdtd51lZmZmeHTTz8t2DUVkskkX3zxhS4eG5H1i3Vw99MsPG8A8PIdXvMe8N4K6pIYc3dqamq4du1aQT+nt7eXzs5O9uzZw+zAVYpFKx9lycbHx2lsbCz4HEA6nV72SkpZGQWDLEkmk+Ho0aOMjo4W5fN6enr44osvNBFZZAoGWZKLFy+uaNnzUs0ttS5WEMksBYPkbHJykqqqqqJPCI6Pj3P27FmNGopIwSA5mbvOQlQLj5qamjRqKCIFgyzK3env76e6ujqyv9rj4+M0NDToBKsiUTDIolKpFL/5zW8iv5BKMQ6RyiwFg9zV3e4NUWw6fFk8Cga5q5s3b/Lb3/42NhN/XV1ddHd3R13GqqdgkDtKpVIcPXo0VreSS6fTVFdXMzU1FXUpq5qCQRbk7ly9epXOzs6oSwnp7Ozk3LlzsRnFrEYKBlnQxMQEJ06ciO3+vG6KW1gKBglJp9PU1dXFYsLxThKJBBcuXIi6jFVLwSAhfX191NTURF3GohoaGjRqKBAFg3zN9PQ01dXVsd2FyJZIJDh58qRub1cACgaZ5+40NzcX9SSplZird2BgIOpSVh0Fg8wbGhrixIkTJTXbn0wmOXPmTEnVXAoUDALM/gc7duxYSa4P6OjoiPVEaSlSMAjuzuXLl+nq6oq6lGWZnp7myJEjur1dHikYhFu3blFVVVXSZy729fXR1tamXYo8UTCscel0mtraWgYHB6MuZUUymQy1tbV5vfHNWqZgWOOuXr1KXd3quO3H4OCgRg15omBYw5LJJNXV1SW9C5Ft7rL2pTiBGjcKhjXK3amrqyvZCcc7GRwc5PPPP181YRcVBcMa5O709fVRXV0ddSkFceHCBRKJRNRllDQFwxrk7qv6mgZTU1PU1dVp1LACCoY1qLm5mUuXLkVdRkFdvHiR/v5+TUQuk4Jhjblx40bJnCS1EpOTkxw9enTV/5yFomBYQzKZDNXV1QwPD0ddSlF0d3evusnVYlEwrBHuTm9vL62trVGXUjTuzpkzZ7ToaRkUDGvE9PQ0R48eXbUTjnfS09PD+fPnNdewRAqGNcDdqaqqore3N+pSii6dTnPy5EnGxsaiLqWkKBjWgFu3bnHu3Lmoy4jM+Pg4TU1NUZdRUhQMq1w6nebIkSNMTExEXUqkzp49q5viLsGiwWBm95lZnZk1m1mLmf1t0L7JzD4zs8vB48as17xrZp1m1mFmrxTyB5A7c3fOnj2rmXlgbGyMs2fPaq4hR7mMGJLAT939GeBZ4FUzewE4ABx39wrgePAcM3sK2Ac8DbwKvG9mZQWoXRbx1VdfUV1drYulBurq6nRT3BwtGgw+a24cek/w5cBrwMGg/SDwerD9GvChuyfd/UugE9ibz6JlcXO3crt582bUpcTG1NQUNTU1Wiqdg5zmGMyszMzOAUPAZ+5eC2x1936A4HFL0H070JP18t6g7Zvv+baZNZhZw1pZcFMs7k53dzcNDQ1RlxI7uilubnIKBndPu/uzwA5gr5n9wV2620JvscB7fuDule5eWV5enlOxkptkMsmRI0e0sGcBMzMza2JJ+Eot6aiEu98CPmd27mDQzLYBBI9DQbdeYGfWy3YA11daqOTG3bl48SJDQ0OLd16jLl++TGNjoyYi7yKXoxLlZvb7wfb9wB8B7cBhYH/QbT/wUbB9GNhnZvea2S6gAlgd1w4rASMjIxw7dizqMmJt7iI1k5OTUZcSW+tz6LMNOBgcWVgHHHL3j82sBjhkZm8B3cAbAO7eYmaHgFZgBnjH3TUtXgS6jHrubty4wYULF/jBD36A2UJ7v2vbosHg7ueB5xZovwG8fIfXvAe8t+LqJGfuTmdnpw7HLUFdXR3f//73efDBB6MuJXa08nGVGB0d5cSJE1qzsASJRIKmpiYdvlyAgmEVmJtpHxkZibqUknPq1CndFHcBCoZV4Nq1azQ2NkZdRkmanp7mzJkzGml9g4KhxM2t5tMv9vJ1dHSU/J248k3BUMLcnaamJjo7O6MupaQlk0mqqqpIJpNRlxIbCoYSdv36dT7//POoy1gV2traaGtri7qM2FAwlKhMJqPrGeZZXV2d1oAEFAwlyN1pbm5eUxd2LYaBgQE6Ojq0VBoFQ0m6ceMGp0+f1oRjnmkU9jsKhhKUTqcpLy9n48aNPPDAA1GXs6oMDAxw6tSpNT9qyOVcCYmZrVu38sYbb5BOpxkbG2NgYIDx8fH5ybORkRGmpqbW/C/3cswd6Xn++ed55JFHoi4nMgqGElVWVkZZWRmbN29m8+bNuDsvvPACADdv3uT27dt0d3czODhIIpFgYGCATCZDKpWKuPL4m5ycpL6+nldeeWXNnmClYFglsn+BN23aBMC3v/1tYHZ1XzKZZHR0lKtXrzI5OTk/yTYxMaF96gWcP3+eyspKNm/eHHUpkbA4DDcrKytdlyErHnefv4LR0NAQY2Nj9PX10dfXx8TExPw5F3H43YjS7t27efPNN1m/fnX8/TSzRnevzKXv6viJZUnMjHvuuQeA7du3s337dvbs2QPA7du3GR0dZXx8nPb2dqanp+nq6prfDVlLR0KuXbtGV1cX3/3ud9fcLoWCQYDf7Yrcf//93H///Tz66KNUVFSQyWTmr3TU39/PyMgIQ0ND9PT0MD09vapv/ZZOp6mpqWH37t3zQbpWKBjkrtatW8dDDz0EQEVFxXxYzAXGwMAAk5OTXLx4kZmZGXp7e8lkMrj7qtgVmRs1zI2o1goFgyzZunXrWLduHQ8//DAPP/ww7s4zzzxDJpMhkUjg7vT399PX18etW7fo7e0lnU6X5ElK7k5NTQ2PPfbYmlozomCQFZvbDSkrK2PuVgBbtmzhmWeeYWZmhmQyyeTkJFeuXCGZTNLS0kI6nSaRSJTE1ZOuXbtGfX09P/7xj9fMXIOCQQpq/fr1rF+/ngcffJDy8nLcnRdffJFMJjM/khgaGpo/jNrf34+7xy4wGhsbefbZZ/nWt74VdSlFoWCQojKz+cVZu3btAuA73/kOP/zhD+cnM6empmhtbSWVSnHp0iXS6TSTk5ORzlmMjY3R1NTESy+9tCZGDQoGiY0NGzbMLyjauXMn7s7LL79MJpOhq6uL6elpEokEXV1dpFIpEolEUes7e/Yse/bs4dFHHy3q50ZBC5ykpMwd7bh9+zYDAwMkk0kuXLhAKpWiu7ubdDo9f9SkEPbs2cMvfvGLklz0pAVOsmqZGWbGAw88wO7duwH43ve+RyaTYXR0lEwmw8jICN3d3YyPj/Pll1/O74rkQ1dXFz09PfO7QauVgkFWhXXr1rFx40YAHnnkEZ588knS6TSpVIpkMsmVK1dIpVLzo4vh4eFlreJMpVLU1tby+OOPs27d6r1qgYJBVq25Sc777ruP5557DnensrISd2dgYICZmRkSiQSdnZ1MTU3R09PztfNI7qSzs5Oenh4ef/zxIv0kxac5BhFmb9ozPj5OMpmkra2NmZkZ2tvbSaVSTExMhOYstm/fzptvvllSt7fTHIPIEq1fv35+V+TRRx/F3XnppZfIZDJcu3aN27dvMzY2Rnt7OzMzM/T399PS0sLevXsjrrwwFAwiCzAzNmzYAMCTTz4JzB4R+dGPfkQqlWJoaKikRgtLpWAQydHcwqYNGzawY8eOiKsprNU7rSoiy6ZgEJEQBYOIhOQcDGZWZmZNZvZx8HyTmX1mZpeDx41Zfd81s04z6zCzVwpRuIgUzlJGDL8Esu/6eQA47u4VwPHgOWb2FLAPeBp4FXjfzMryU66IFENOwWBmO4A/Bv4hq/k14GCwfRB4Pav9Q3dPuvuXQCewOg/2iqxSuY4Y/h74ayB7+ddWd+8HCB63BO3bgZ6sfr1B29eY2dtm1mBmDcPDw0utW0QKaNFgMLM/AYbcvTHH91zoKhahddfu/oG7V7p75dzlwEQkHnJZ4PQi8Kdm9nPgPuBhM/tnYNDMtrl7v5ltA4aC/r3AzqzX7wCu57NoESmsRUcM7v6uu+9w9yeYnVQ84e5/DhwG9gfd9gMfBduHgX1mdq+Z7QIqgLq8Vy4iBbOSJdG/Ag6Z2VtAN/AGgLu3mNkhoBWYAd5x97Vz+yKRVUCnXYusEUs57VorH0UkRMEgIiEKBhEJUTCISIiCQURCFAwiEqJgEJEQBYOIhCgYRCREwSAiIQoGEQlRMIhIiIJBREIUDCISomAQkRAFg4iEKBhEJETBICIhCgYRCVEwiEiIgkFEQhQMIhKiYBCREAWDiIQoGEQkRMEgIiEKBhEJUTCISIiCQURCFAwiEqJgEJEQBYOIhCgYRCREwSAiITkFg5ldNbMLZnbOzBqCtk1m9pmZXQ4eN2b1f9fMOs2sw8xeKVTxIlIYSxkx/KG7P+vulcHzA8Bxd68AjgfPMbOngH3A08CrwPtmVpbHmkWkwFayK/EacDDYPgi8ntX+obsn3f1LoBPYu4LPEZEiyzUYHDhqZo1m9nbQttXd+wGCxy1B+3agJ+u1vUHb15jZ22bWYGYNw8PDy6teRApifY79XnT362a2BfjMzNrv0tcWaPNQg/sHwAcAlZWVoe+LSHRyGjG4+/XgcQj4T2Z3DQbNbBtA8DgUdO8Fdma9fAdwPV8Fi0jhLRoMZvagmf3e3Dbw34GLwGFgf9BtP/BRsH0Y2Gdm95rZLqACqMt34SJSOLnsSmwF/tPM5vr/q7t/amb1wCEzewvoBt4AcPcWMzsEtAIzwDvuni5I9SJSEOYe/e69mQ0DXwEjUdeSg82oznwrlVpLpU5YuNbH3b08lxfHIhgAzKwha41EbKnO/CuVWkulTlh5rVoSLSIhCgYRCYlTMHwQdQE5Up35Vyq1lkqdsMJaYzPHICLxEacRg4jEROTBYGavBqdnd5rZgRjU82szGzKzi1ltsTvF3Mx2mtl/mVmbmbWY2S/jWKuZ3WdmdWbWHNT5t3GsM+uzy8ysycw+jnmdhb0UgrtH9gWUAV3AbmAD0Aw8FXFNPwaeBy5mtf0/4ECwfQD4v8H2U0HN9wK7gp+lrEh1bgOeD7Z/D7gU1BOrWpk9d+ahYPseoBZ4IW51ZtX7l8C/Ah/H9d8++PyrwOZvtOWt1qhHDHuBTne/4u7TwIfMnrYdGXc/BSS+0Ry7U8zdvd/dzwbb40Abs2exxqpWnzURPL0n+PK41QlgZjuAPwb+Ias5dnXeRd5qjToYcjpFOwZWdIp5oZnZE8BzzP41jl2twfD8HLMn2n3m7rGsE/h74K+BTFZbHOuEAlwKIVuup10XSk6naMdY5PWb2UPAvwN/4e5jwTktC3ZdoK0otfrsuTLPmtnvM3vezR/cpXskdZrZnwBD7t5oZj/J5SULtBXz3z7vl0LIFvWIoVRO0Y7lKeZmdg+zofAv7v4fca4VwN1vAZ8ze8m/uNX5IvCnZnaV2V3an5rZP8ewTqDwl0KIOhjqgQoz22VmG5i9VuThiGtaSOxOMbfZocE/Am3u/ndxrdXMyoORAmZ2P/BHQHvc6nT3d919h7s/wezv4Ql3//O41QlFuhRCsWZR7zK7+nNmZ9S7gL+JQT3/BvQDKWaT9i3gEWYveHs5eNyU1f9vgto7gP9RxDp/xOxw8DxwLvj6edxqBf4b0BTUeRH430F7rOr8Rs0/4XdHJWJXJ7NH8ZqDr5a5/zf5rFUrH0UkJOpdCRGJIQWDiIQoGEQkRMEgIiEKBhEJUTCISIiCQURCFAwiEvL/AYkyQBP/wl2/AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plt.imshow(layer_imgs[0][2])\n",
    "plt.imshow(layer_imgs[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56b489f3-4420-48e6-81b3-5ed9846fc792",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also need to generate some utils for the training for the layering maps\n",
    "# For example need some code that combines all layer maps below a layer and all above a layer\n",
    "# Also need analagous code that does this for the layered  objects in the image\n",
    "\n",
    "def get_front_layers(layer_imgs, layer_bin, layer_no):\n",
    "    num_layers = len(layer_imgs) -1 #subtract 1 for loops??\n",
    "    front_layers = np.ones(layer_imgs[0].shape)\n",
    "    front_bin = np.zeros(layer_bin.shape)\n",
    "    if layer_no == num_layers:\n",
    "        return front_layers,front_bin\n",
    "    else:\n",
    "        for layer_idx in range(layer_no,num_layers):\n",
    "            front_layers[layer_bin==1,:] = layer_imgs[layer_bin==1,:]\n",
    "            front_bin[layer_bin==1] = 1\n",
    "        return front_layers,front_bin\n",
    "        \n",
    "\n",
    "def get_back_layer(layer_imgs, layer_bin, layer_no):\n",
    "    num_layers = len(layer_imgs) - 1\n",
    "    back_layers = np.ones(layer_imgs[0].shape)\n",
    "    back_bin = np.zeros(layer_bin.shape)\n",
    "    for layer_idx in range(layer_no):\n",
    "        #FINISH / FIX THIS CODE AT SOMEPOINT\n",
    "        front_layers[layer_bin==1,:] = layer_imgs[layer_bin==1,:]\n",
    "        front_bin[layer_bin==1] = 1\n",
    "    return front_layers,front_bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1817a7a-6ae8-4c74-93fe-dc36f318696f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "## Network that takes 4 dimensional input and pushes it to 3 dimensions for the vae\n",
    "class ContractNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ContractNet, self).__init__()\n",
    "        self.conv = nn.Conv2d(4, 3, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "    \n",
    "\n",
    "## Network that takes 3 dimensional input and pushes it to 4 dimensions for model out\n",
    "class ExpandNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ExpandNet, self).__init__()\n",
    "        self.conv = nn.Conv2d(3, 4, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d58bbfc-ec06-42ee-8f25-0e7b5b22e583",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n"
     ]
    }
   ],
   "source": [
    "#Instantiate the two adapters and the pretrained model\n",
    "\n",
    "# An adapter that takes in the original image (or a subset of layers)\n",
    "RGB_adapter = T2IAdapter(channels_in=int(3), \n",
    "                       block_out_channels=[320, 640, 1280, 1280][:4], \n",
    "                       num_res_blocks=2, \n",
    "                       kernel_size=1, \n",
    "                       res_block_skip=True, \n",
    "                       use_conv=False)\n",
    "\n",
    "# An adapter that takes in the mask given by the layers above the current one\n",
    "mask_adapter = T2IAdapter(channels_in=int(1), \n",
    "                       block_out_channels=[320, 640, 1280, 1280][:4], \n",
    "                       num_res_blocks=2, \n",
    "                       kernel_size=1, \n",
    "                       res_block_skip=True, \n",
    "                       use_conv=False)\n",
    "\n",
    "#Combine them into a single adapter\n",
    "# adapter = MultiAdapter([RGB_adapter,mask_adapter])\n",
    "adapter = RGB_adapter\n",
    "\n",
    "#Instantiate the Convolutional layers\n",
    "contract_layer = ContractNet()\n",
    "expand_layer = ExpandNet()\n",
    "\n",
    "#Pretrained stable diffusion model that we will try not to touch (may end up changing the final conv_out though.\n",
    "#To be honest I am really hoping this works\n",
    "\n",
    "model_name = \"runwayml/stable-diffusion-v1-5\"\n",
    "pipe = StableDiffusionAdapterPipeline.from_pretrained(model_name, torch_dtype=torch.float32).to('cuda')\n",
    "\n",
    "vae = pipe.vae\n",
    "unet = pipe.unet\n",
    "# also get the clip model because it matters probably....\n",
    "\n",
    "#But looks like it may not work as just the model for training so we have to separate it into parts for training anyway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9af19c60-c4a6-4d3e-aef4-072f6b489d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_scheduler = DDPMScheduler.from_pretrained(model_name, subfolder=\"scheduler\")\n",
    "# text_encoder = text_encoder_cls.from_pretrained(\n",
    "    # args.pretrained_model_name_or_path, subfolder=\"text_encoder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3b858c58-d6e5-4006-8532-a35066896acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random point, what if I dont do anything about the masking etc, and just try and make a model that removes the top object in a scene, or like just tries to get the layer mappings?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b236bcea-8337-4368-8d29-c1e274e13a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def make_train_dataset(args):\n",
    "    # Get the datasets: you can either provide your own training and evaluation files (see below)\n",
    "    # or specify a Dataset from the hub (the dataset will be downloaded automatically from the datasets Hub).\n",
    "\n",
    "    # In distributed training, the load_dataset function guarantees that only one local process can concurrently\n",
    "    # download the dataset.\n",
    "    if args.dataset_name is not None:\n",
    "        # Downloading and loading a dataset from the hub.\n",
    "        dataset = load_dataset(\n",
    "            args.dataset_name,\n",
    "            args.dataset_config_name,\n",
    "            cache_dir=args.cache_dir,\n",
    "        )\n",
    "    else:\n",
    "        if args.train_data_dir is not None:\n",
    "            dataset = load_dataset(\n",
    "                args.train_data_dir,\n",
    "                cache_dir=args.cache_dir,\n",
    "            )\n",
    "        # See more about loading custom images at\n",
    "        # https://huggingface.co/docs/datasets/v2.0.0/en/dataset_script\n",
    "\n",
    "    # Preprocessing the datasets.\n",
    "    # We need to tokenize inputs and targets.\n",
    "    column_names = dataset[\"train\"].column_names\n",
    "\n",
    "    # 6. Get the column names for input/target.\n",
    "    if args.image_column is None:\n",
    "        image_column = column_names[0]\n",
    "        print(f\"image column defaulting to {image_column}\")\n",
    "    else:\n",
    "        image_column = args.image_column\n",
    "        if image_column not in column_names:\n",
    "            raise ValueError(\n",
    "                f\"`--image_column` value '{args.image_column}' not found in dataset columns. Dataset columns are: {', '.join(column_names)}\"\n",
    "            )\n",
    "\n",
    "    if args.caption_column is None:\n",
    "        caption_column = column_names[1]\n",
    "        print(f\"caption column defaulting to {caption_column}\")\n",
    "    else:\n",
    "        caption_column = args.caption_column\n",
    "        if caption_column not in column_names:\n",
    "            raise ValueError(\n",
    "                f\"`--caption_column` value '{args.caption_column}' not found in dataset columns. Dataset columns are: {', '.join(column_names)}\"\n",
    "            )\n",
    "\n",
    "    if args.conditioning_image_column is None:\n",
    "        conditioning_image_column = column_names[2]\n",
    "        print(f\"conditioning image column defaulting to {conditioning_image_column}\")\n",
    "    else:\n",
    "        conditioning_image_column = args.conditioning_image_column\n",
    "        if conditioning_image_column not in column_names:\n",
    "            raise ValueError(\n",
    "                f\"`--conditioning_image_column` value '{args.conditioning_image_column}' not found in dataset columns. Dataset columns are: {', '.join(column_names)}\"\n",
    "            )\n",
    "\n",
    "    # def tokenize_captions(examples, is_train=True):\n",
    "    #     captions = []\n",
    "    #     for caption in examples[caption_column]:\n",
    "    #         if random.random() < args.proportion_empty_prompts:\n",
    "    #             captions.append(\"\")\n",
    "    #         elif isinstance(caption, str):\n",
    "    #             captions.append(caption)\n",
    "    #         elif isinstance(caption, (list, np.ndarray)):\n",
    "    #             # take a random caption if there are multiple\n",
    "    #             captions.append(random.choice(caption) if is_train else caption[0])\n",
    "    #         else:\n",
    "    #             raise ValueError(\n",
    "    #                 f\"Caption column `{caption_column}` should contain either strings or lists of strings.\"\n",
    "    #             )\n",
    "    #     inputs = tokenizer(\n",
    "    #         captions, max_length=tokenizer.model_max_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\"\n",
    "    #     )\n",
    "    #     return inputs.input_ids\n",
    "\n",
    "    image_transforms = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize(args.resolution, interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "            transforms.CenterCrop(args.resolution),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.5], [0.5]),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    conditioning_image_transforms = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize(args.resolution, interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "            transforms.CenterCrop(args.resolution),\n",
    "            transforms.ToTensor(),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    def preprocess_train(examples):\n",
    "        images = [image.convert(\"RGB\") for image in examples[image_column]]\n",
    "        images = [image_transforms(image) for image in images]\n",
    "\n",
    "        conditioning_images = [image.convert(\"RGB\") for image in examples[conditioning_image_column]]\n",
    "        conditioning_images = [conditioning_image_transforms(image) for image in conditioning_images]\n",
    "\n",
    "        examples[\"pixel_values\"] = images\n",
    "        examples[\"conditioning_pixel_values\"] = conditioning_images\n",
    "        # examples[\"input_ids\"] = tokenize_captions(examples)\n",
    "\n",
    "        return examples\n",
    "\n",
    "    # with accelerator.main_process_first():\n",
    "    if args.max_train_samples is not None:\n",
    "        dataset[\"train\"] = dataset[\"train\"].shuffle(seed=args.seed).select(range(args.max_train_samples))\n",
    "    # Set the training transforms\n",
    "    train_dataset = dataset[\"train\"].with_transform(preprocess_train)\n",
    "\n",
    "    return train_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8e957bd7-8c7b-4ae8-9be2-420ffca57387",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">NameError: </span>name <span style=\"color: #008000; text-decoration-color: #008000\">'dataset'</span> is not defined\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92m<module>\u001b[0m:\u001b[94m1\u001b[0m                                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mNameError: \u001b[0mname \u001b[32m'dataset'\u001b[0m is not defined\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5557aab5-a6f6-4505-a202-1bd965204624",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">NameError: </span>name <span style=\"color: #008000; text-decoration-color: #008000\">'load_dataset'</span> is not defined\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92m<module>\u001b[0m:\u001b[94m1\u001b[0m                                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mNameError: \u001b[0mname \u001b[32m'load_dataset'\u001b[0m is not defined\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(args.dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b63510d6-3c08-4ad0-b0af-2f02424ff3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from types import SimpleNamespace\n",
    "from datasets import load_dataset\n",
    "\n",
    "args = SimpleNamespace()\n",
    "\n",
    "args.dataset_name = \"fusing/fill50k\"\n",
    "args.train_data_dir = None #\"../ControlNet/training/fill50k\"\n",
    "args.dataset_config_name=None\n",
    "args.cache_dir = None\n",
    "args.image_column = \"image\"\n",
    "args.caption_column = \"text\"\n",
    "args.conditioning_image_column = \"conditioning_image\"\n",
    "args.resolution = 512\n",
    "args.max_train_samples = 1000\n",
    "args.seed = None\n",
    "args.train_batch_size = 4\n",
    "args.dataloader_num_workers = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136a0eaa-7cd4-40b7-8c01-75b638a2df2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a583745e-183d-4928-b691-a3dd9d1dc58e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No config specified, defaulting to: fill50k/default\n",
      "Found cached dataset fill50k (/n/home07/adamaraju/.cache/huggingface/datasets/fusing___fill50k/default/0.0.2/f23b778406682a796a540934e7163495e1b8a88fefc76ca08f7e5a79ddcd668b)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d35b6f63f5f947bfa4679367418389e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_data = make_train_dataset(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9500485b-db98-40f8-8b1a-fceb7fa42cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(examples):\n",
    "    pixel_values = torch.stack([example[\"pixel_values\"] for example in examples])\n",
    "    pixel_values = pixel_values.to(memory_format=torch.contiguous_format).float()\n",
    "\n",
    "    conditioning_pixel_values = torch.stack([example[\"conditioning_pixel_values\"] for example in examples])\n",
    "    conditioning_pixel_values = conditioning_pixel_values.to(memory_format=torch.contiguous_format).float()\n",
    "\n",
    "    # input_ids = torch.stack([example[\"input_ids\"] for example in examples])\n",
    "\n",
    "    return {\n",
    "        \"pixel_values\": pixel_values,\n",
    "        \"conditioning_pixel_values\": conditioning_pixel_values,\n",
    "        # \"input_ids\": input_ids,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fb51a592-4207-4872-b239-706e989d8778",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_data,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn,\n",
    "    batch_size=args.train_batch_size,\n",
    "    num_workers=args.dataloader_num_workers,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0eae734f-456b-41c0-89dd-03a835043155",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3, 512, 512])\n"
     ]
    }
   ],
   "source": [
    "for step, batch in enumerate(train_dataloader):\n",
    "    print(batch[\"pixel_values\"].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0bdde994-b42d-4083-b329-0987bc25e9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instantiate the Convolutional layers\n",
    "\n",
    "contract_layer = ContractNet().to(\"cuda\")\n",
    "expand_layer = ExpandNet().to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d368b2fc-132c-4262-819e-7a73c2290482",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set optimizer class\n",
    "# params_to_optimize = list(contract_layer.parameters()) + list(expand_layer.parameters()) + list(adapter.parameters())\n",
    "params_to_optimize =adapter.parameters()\n",
    "\n",
    "optimizer_class = torch.optim.AdamW\n",
    "optimizer = optimizer_class(\n",
    "    params_to_optimize,\n",
    "    lr = 1e-3,#lr=args.learning_rate,\n",
    "    #betas=(args.adam_beta1, args.adam_beta2),\n",
    "    #weight_decay=args.adam_weight_decay,\n",
    "    #eps=args.adam_epsilon,\n",
    ") #put args/ optimizer class here!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0be22739-8a74-4b9b-8be2-07074ad8a68f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">TypeError: </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">imshow</span><span style=\"font-weight: bold\">()</span> missing <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> required positional argument: <span style=\"color: #008000; text-decoration-color: #008000\">'X'</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92m<module>\u001b[0m:\u001b[94m1\u001b[0m                                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mTypeError: \u001b[0m\u001b[1;35mimshow\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m missing \u001b[1;36m1\u001b[0m required positional argument: \u001b[32m'X'\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5322aa2b-1954-43e4-b0a5-bcf7d52238d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test1 = torch.tensor(layer_imgs[0][1]).permute(2,0,1)\n",
    "test2 = torch.tensor(layer_binaries[0][...,0]).unsqueeze(0)\n",
    "test_T = torch.cat([test1,test2],axis=0).to(\"cuda:0\")\n",
    "test_T = test_T.unsqueeze(0)\n",
    "weight_dtype = torch.float32\n",
    "# contract_in = contract_layer(test_T.to(dtype=weight_dtype))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a174631f-a499-458f-86c1-6475080b18e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "bsz = 4\n",
    "val1 = test1.unsqueeze(0).to('cuda').repeat(bsz,1,1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "65456b1e-d08a-428c-87ba-e5d64400e870",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3, 512, 512])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a7050ce7-a506-40d5-9243-6cbe389d3b3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">NameError: </span>name <span style=\"color: #008000; text-decoration-color: #008000\">'in1'</span> is not defined\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92m<module>\u001b[0m:\u001b[94m1\u001b[0m                                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mNameError: \u001b[0mname \u001b[32m'in1'\u001b[0m is not defined\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "in1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d5ffb558-9ab6-498d-8f4c-da81aec741dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "condition_img = torch.tensor(base_imgs[0]).permute(2,0,1).unsqueeze(0).to(torch.float)\n",
    "condition_mask = torch.tensor(layer_binaries[0][...,-1]).unsqueeze(0).unsqueeze(0).to(torch.float)\n",
    "# conditional_in = torch.cat([in1,in2],axis=0).to(\"cuda:0\")\n",
    "# conditional_in = conditional_in.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "85e95690-4746-461f-94e1-c92b63c6eff6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">NameError: </span>name <span style=\"color: #008000; text-decoration-color: #008000\">'conditional_in'</span> is not defined\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92m<module>\u001b[0m:\u001b[94m1\u001b[0m                                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mNameError: \u001b[0mname \u001b[32m'conditional_in'\u001b[0m is not defined\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "conditional_in.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3cc98e47-c450-41ab-aa5c-45f13d0c68d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">NameError: </span>name <span style=\"color: #008000; text-decoration-color: #008000\">'contract_in'</span> is not defined\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92m<module>\u001b[0m:\u001b[94m1\u001b[0m                                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mNameError: \u001b[0mname \u001b[32m'contract_in'\u001b[0m is not defined\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "contract_in.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "53c309ae-3217-4bcb-ae69-f99b700afd68",
   "metadata": {},
   "outputs": [],
   "source": [
    "latents = vae.encode(val1.to(dtype=weight_dtype)).latent_dist.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b29119b0-1011-478f-bcbb-d99fc6d85402",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 4, 64, 64])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latents.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "78c9279f-80b0-4ffc-be95-f55271e219c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample noise that we'll add to the latents\n",
    "noise = torch.randn_like(latents)\n",
    "bsz = latents.shape[0]\n",
    "# Sample a random timestep for each image\n",
    "timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps, (bsz,), device=latents.device)\n",
    "timesteps = timesteps.long()\n",
    "\n",
    "# Add noise to the latents according to the noise magnitude at each timestep\n",
    "# (this is the forward diffusion process)\n",
    "noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d7abc79a-4b39-4bb4-a3d9-be69959c88fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 4, 64, 64])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noisy_latents.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2644ee52-4c21-4ca1-8ff4-58d08e361a55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 512, 512])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "condition_img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5a9437a3-fb02-4f63-a1f5-322eff3ecddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "adapter_input = condition_img.repeat(bsz,1,1,1)\n",
    "\n",
    "#MultiAdapter stuff\n",
    "# n, c, h, w = adapter_input[0].shape\n",
    "# adapter_input = torch.stack([x.reshape([n * x.shape[1], h, w]) for x in adapter_input])\n",
    "\n",
    "#CHANGE THE CODE SO I CAN GIVE IT A LIST OF adapters or just slice correctly\n",
    "\n",
    "adapter_state = adapter(adapter_input)\n",
    "# len(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f7186daa-a02e-4642-a7d3-48c40617201c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3, 512, 512])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adapter_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e1b014b2-cf99-4faf-b320-05523bdd2d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "do_classifier_free_guidance= False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e54d453e-b237-4346-a54f-4f33635ae1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in enumerate(adapter_state):\n",
    "    adapter_state[k] = v.to(\"cuda\")#* adapter_conditioning_scale\n",
    "if do_classifier_free_guidance:\n",
    "    for k, v in enumerate(adapter_state):\n",
    "        adapter_state[k] = torch.cat([v] * 2, dim=0).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "eef28322-87f1-4e46-96a6-425d68db57a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "noisy_latents = torch.cat([noisy_latents] * 2) if do_classifier_free_guidance else noisy_latents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ac5a898d-8ea6-4db6-bbb0-542f3280b9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # adapter_state = adapter(adapter_input)\n",
    "# for k, v in enumerate(adapter_state):\n",
    "#     adapter_state[k] = v * adapter_conditioning_scale\n",
    "# if num_images_per_prompt > 1:\n",
    "#     for k, v in enumerate(adapter_state):\n",
    "#         adapter_state[k] = v.repeat(num_images_per_prompt, 1, 1, 1)\n",
    "# if do_classifier_free_guidance:\n",
    "#     for k, v in enumerate(adapter_state):\n",
    "#         adapter_state[k] = torch.cat([v] * 2, dim=0)\n",
    "\n",
    "# # ## expand the latents if we are doing classifier free guidance\n",
    "# # noisy_latents = torch.cat([noisy_latents] * 2) if do_classifier_free_guidance else latents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e99179d7-1ea8-446f-8212-41fb6a9cba54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adapter_state[k].device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f0202209-0094-4b91-929d-f90a4b34e79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_embeds = pipe._encode_prompt(\n",
    "    \"\",\n",
    "    device=\"cuda\",\n",
    "    num_images_per_prompt = bsz ,\n",
    "    do_classifier_free_guidance=do_classifier_free_guidance,\n",
    "    negative_prompt=None,\n",
    "    prompt_embeds=None,\n",
    "    negative_prompt_embeds=None,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7beb53fd-282b-4103-bc97-cc01f378c697",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 77, 768])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_embeds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fd8aa7ca-6dd8-4ab2-9163-0f1496222249",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 4, 64, 64])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noisy_latents.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1e504bdf-9f75-4911-8ff7-c4eb7daf5efe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 1280, 8, 8])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " adapter_state[k].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4c0e8f2b-a146-4d91-9c38-be7798c2b683",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timesteps.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "376b77ad-8b6d-4797-9df1-92e80dff33f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adapter_state[k].device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d1d81d09-b9d6-4b77-993b-f5f73bfe3f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    noise_pred = unet(\n",
    "            noisy_latents,\n",
    "            timesteps,\n",
    "            encoder_hidden_states=prompt_embeds,\n",
    "            cross_attention_kwargs=None,\n",
    "            down_block_additional_residuals=[state.clone() for state in adapter_state],\n",
    "    ).sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "8c94195b-17d0-4ff7-82dc-16498702a4aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noisy_latents.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "34befc87-861d-4339-9805-06e25f1a24c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 64, 64])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noise_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "f7cd6cc3-b7cc-4811-9df9-44836980a235",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-147-7f51ce7b94b8>:2: UserWarning: Using a target size (torch.Size([1, 4, 64, 64])) that is different to the input size (torch.Size([2, 4, 64, 64])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = F.mse_loss(noise_pred.float(), noise.float(), reduction=\"mean\")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "loss = F.mse_loss(noise_pred.float(), noise.float(), reduction=\"mean\")\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e7875a7-36e7-479b-9d30-20249fe0fed8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">5</span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">NameError: </span>name <span style=\"color: #008000; text-decoration-color: #008000\">'adapter'</span> is not defined\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92m<module>\u001b[0m:\u001b[94m5\u001b[0m                                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mNameError: \u001b[0mname \u001b[32m'adapter'\u001b[0m is not defined\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Set optimizer class\n",
    "optimizer_class = torch.optim.AdamW\n",
    "\n",
    "#Get parameters to optimize\n",
    "params_to_optimize = list(contract_layer.parameters()) + list(expand_layer.parameters()) + list(adapter.parameters())\n",
    "optimizer = optimizer_class(\n",
    "    params_to_optimize,\n",
    "    lr = 1e-3,#lr=args.learning_rate,\n",
    "    #betas=(args.adam_beta1, args.adam_beta2),\n",
    "    #weight_decay=args.adam_weight_decay,\n",
    "    #eps=args.adam_epsilon,\n",
    ") #put args/ optimizer class here!!\n",
    "\n",
    "# Also add two tiny networks that are a conv2D with 1 kernel size and 1 stride, going from 3 to 4 dimensions\n",
    "# Pipeline\n",
    "### IN PARALLEL: turn image + mask into 4d mask -> 4d mask to 3d using convolution -> 3d to latent with vae\n",
    "### IN PARALLEL: layer image + other mask into 2 part adapter\n",
    "### get unet and put both models into this\n",
    "## back prop and thats it!!\n",
    "\n",
    "\n",
    "# additional, try to figure out accelerator\n",
    "# additional work on getting saving for outputs, see if i can get it to work..\n",
    "\n",
    "\n",
    "# Train!\n",
    "total_batch_size = args.train_batch_size * accelerator.num_processes * args.gradient_accumulation_steps\n",
    "\n",
    "logger.info(\"***** Running training *****\")\n",
    "logger.info(f\"  Num examples = {len(train_dataset)}\")\n",
    "logger.info(f\"  Num batches each epoch = {len(train_dataloader)}\")\n",
    "logger.info(f\"  Num Epochs = {args.num_train_epochs}\")\n",
    "logger.info(f\"  Instantaneous batch size per device = {args.train_batch_size}\")\n",
    "logger.info(f\"  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}\")\n",
    "logger.info(f\"  Gradient Accumulation steps = {args.gradient_accumulation_steps}\")\n",
    "logger.info(f\"  Total optimization steps = {args.max_train_steps}\")\n",
    "global_step = 0\n",
    "first_epoch = 0\n",
    "\n",
    "# Potentially load in the weights and states from a previous save\n",
    "if args.resume_from_checkpoint:\n",
    "    if args.resume_from_checkpoint != \"latest\":\n",
    "        path = os.path.basename(args.resume_from_checkpoint)\n",
    "    else:\n",
    "        # Get the most recent checkpoint\n",
    "        dirs = os.listdir(args.output_dir)\n",
    "        dirs = [d for d in dirs if d.startswith(\"checkpoint\")]\n",
    "        dirs = sorted(dirs, key=lambda x: int(x.split(\"-\")[1]))\n",
    "        path = dirs[-1] if len(dirs) > 0 else None\n",
    "\n",
    "    if path is None:\n",
    "        accelerator.print(\n",
    "            f\"Checkpoint '{args.resume_from_checkpoint}' does not exist. Starting a new training run.\"\n",
    "        )\n",
    "        args.resume_from_checkpoint = None\n",
    "        initial_global_step = 0\n",
    "    else:\n",
    "        accelerator.print(f\"Resuming from checkpoint {path}\")\n",
    "        accelerator.load_state(os.path.join(args.output_dir, path))\n",
    "        global_step = int(path.split(\"-\")[1])\n",
    "\n",
    "        initial_global_step = global_step\n",
    "        first_epoch = global_step // num_update_steps_per_epoch\n",
    "else:\n",
    "    initial_global_step = 0\n",
    "\n",
    "progress_bar = tqdm(\n",
    "    range(0, args.max_train_steps),\n",
    "    initial=initial_global_step,\n",
    "    desc=\"Steps\",\n",
    "    # Only show the progress bar once on each machine.\n",
    "    # disable=not accelerator.is_local_main_process,\n",
    ")\n",
    "\n",
    "weight_dtype = torch.float32\n",
    "image_logs = None\n",
    "for epoch in range(first_epoch, args.num_train_epochs):\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        with accelerator.accumulate(controlnet):\n",
    "            #### Convert images to latent space\n",
    "            #network that goes from four to three dimensions\n",
    "            contract_in = contract_layer(batch[\"pixel_values\"].to(dtype=weight_dtype))\n",
    "            latents = vae.encode(contract_in).latent_dist.sample()\n",
    "            latents = latents * vae.config.scaling_factor\n",
    "            \n",
    "            \n",
    "            # 5. Prepare latent variables\n",
    "            # one possible option\n",
    "            # latents = vae.encode(batch[\"pixel_values\"].to(dtype=weight_dtype)).latent_dist.sample()\n",
    "            # latents = latents * vae.config.scaling_factor\n",
    "            # \n",
    "            \n",
    "            # num_channels_latents = self.unet.in_channels + 1 ##ADDED 1 ( 3->4)\n",
    "#             latents = self.prepare_latents(\n",
    "#                 batch_size * num_images_per_prompt,\n",
    "#                 num_channels_latents,\n",
    "#                 height,\n",
    "#                 width,\n",
    "#                 prompt_embeds.dtype,\n",
    "#                 device,\n",
    "#                 generator,\n",
    "#                 latents,\n",
    "#             )\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            # Sample noise that we'll add to the latents\n",
    "            noise = torch.randn_like(latents)\n",
    "            bsz = latents.shape[0]\n",
    "            # Sample a random timestep for each image\n",
    "            timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps, (bsz,), device=latents.device)\n",
    "            timesteps = timesteps.long()\n",
    "\n",
    "            # Add noise to the latents according to the noise magnitude at each timestep\n",
    "            # (this is the forward diffusion process)\n",
    "            noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n",
    "\n",
    "            # Get the text embedding for conditioning\n",
    "            encoder_hidden_states = text_encoder(batch[\"input_ids\"])[0]\n",
    "\n",
    "            # controlnet_image = batch[\"conditioning_pixel_values\"].to(dtype=weight_dtype)\n",
    "\n",
    "#             down_block_res_samples, mid_block_res_sample = controlnet(\n",
    "#                 noisy_latents,\n",
    "#                 timesteps,\n",
    "#                 encoder_hidden_states=encoder_hidden_states,\n",
    "#                 controlnet_cond=controlnet_image,\n",
    "#                 return_dict=False,\n",
    "#             )\n",
    "\n",
    "#             # Predict the noise residual\n",
    "#             model_pred = unet(\n",
    "#                 noisy_latents,\n",
    "#                 timesteps,\n",
    "#                 encoder_hidden_states=encoder_hidden_states,\n",
    "#                 down_block_additional_residuals=[\n",
    "#                     sample.to(dtype=weight_dtype) for sample in down_block_res_samples\n",
    "#                 ],\n",
    "#                 mid_block_additional_residual=mid_block_res_sample.to(dtype=weight_dtype),\n",
    "#             ).sample\n",
    "            \n",
    "    \n",
    "            # 7. Denoising loop\n",
    "            adapter_state = self.adapter(adapter_input)\n",
    "            for k, v in enumerate(adapter_state):\n",
    "                adapter_state[k] = v * adapter_conditioning_scale\n",
    "            if num_images_per_prompt > 1:\n",
    "                for k, v in enumerate(adapter_state):\n",
    "                    adapter_state[k] = v.repeat(num_images_per_prompt, 1, 1, 1)\n",
    "            if do_classifier_free_guidance:\n",
    "                for k, v in enumerate(adapter_state):\n",
    "                    adapter_state[k] = torch.cat([v] * 2, dim=0)\n",
    "\n",
    "            ## expand the latents if we are doing classifier free guidance\n",
    "            latent_model_input = torch.cat([latents] * 2) if do_classifier_free_guidance else latents\n",
    "\n",
    "            # predict the noise residual\n",
    "            noise_pred = unet(\n",
    "                    latent_model_input,\n",
    "                    t,\n",
    "                    encoder_hidden_states=prompt_embeds,\n",
    "                    cross_attention_kwargs=cross_attention_kwargs,\n",
    "                    down_block_additional_residuals=[state.clone() for state in adapter_state],\n",
    "            ).sample\n",
    "    \n",
    "            # Add something here that is consistent with increasing the dimensionality of the model from three to four dimensions\n",
    "            # something like noisy_out = depth_conv(noise_pred)\n",
    "            \n",
    "            \n",
    "            # Get the target for loss depending on the prediction type\n",
    "            if noise_scheduler.config.prediction_type == \"epsilon\":\n",
    "                target = noise\n",
    "            elif noise_scheduler.config.prediction_type == \"v_prediction\":\n",
    "                target = noise_scheduler.get_velocity(latents, noise, timesteps)\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown prediction type {noise_scheduler.config.prediction_type}\")\n",
    "            loss = F.mse_loss(model_pred.float(), target.float(), reduction=\"mean\")\n",
    "\n",
    "            # loss backward\n",
    "            # accelerator.backward(loss)\n",
    "            # if accelerator.sync_gradients:\n",
    "            #     params_to_clip = controlnet.parameters()\n",
    "            #     accelerator.clip_grad_norm_(params_to_clip, args.max_grad_norm)\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad(set_to_none=args.set_grads_to_none)\n",
    "\n",
    "        # Checks if the accelerator has performed an optimization step behind the scenes\n",
    "        if accelerator.sync_gradients:\n",
    "            progress_bar.update(1)\n",
    "            global_step += 1\n",
    "\n",
    "            if accelerator.is_main_process:\n",
    "                if global_step % args.checkpointing_steps == 0:\n",
    "                    # _before_ saving state, check if this save would set us over the `checkpoints_total_limit`\n",
    "                    if args.checkpoints_total_limit is not None:\n",
    "                        checkpoints = os.listdir(args.output_dir)\n",
    "                        checkpoints = [d for d in checkpoints if d.startswith(\"checkpoint\")]\n",
    "                        checkpoints = sorted(checkpoints, key=lambda x: int(x.split(\"-\")[1]))\n",
    "\n",
    "                        # before we save the new checkpoint, we need to have at _most_ `checkpoints_total_limit - 1` checkpoints\n",
    "                        if len(checkpoints) >= args.checkpoints_total_limit:\n",
    "                            num_to_remove = len(checkpoints) - args.checkpoints_total_limit + 1\n",
    "                            removing_checkpoints = checkpoints[0:num_to_remove]\n",
    "\n",
    "                            logger.info(\n",
    "                                f\"{len(checkpoints)} checkpoints already exist, removing {len(removing_checkpoints)} checkpoints\"\n",
    "                            )\n",
    "                            logger.info(f\"removing checkpoints: {', '.join(removing_checkpoints)}\")\n",
    "\n",
    "                            for removing_checkpoint in removing_checkpoints:\n",
    "                                removing_checkpoint = os.path.join(args.output_dir, removing_checkpoint)\n",
    "                                shutil.rmtree(removing_checkpoint)\n",
    "\n",
    "                    save_path = os.path.join(args.output_dir, f\"checkpoint-{global_step}\")\n",
    "                    accelerator.save_state(save_path)\n",
    "                    logger.info(f\"Saved state to {save_path}\")\n",
    "\n",
    "                if args.validation_prompt is not None and global_step % args.validation_steps == 0:\n",
    "                    image_logs = log_validation(\n",
    "                        vae,\n",
    "                        text_encoder,\n",
    "                        tokenizer,\n",
    "                        unet,\n",
    "                        controlnet,\n",
    "                        args,\n",
    "                        accelerator,\n",
    "                        weight_dtype,\n",
    "                        global_step,\n",
    "                    )\n",
    "\n",
    "        logs = {\"loss\": loss.detach().item(), \"lr\": lr_scheduler.get_last_lr()[0]}\n",
    "        progress_bar.set_postfix(**logs)\n",
    "        accelerator.log(logs, step=global_step)\n",
    "\n",
    "        if global_step >= args.max_train_steps:\n",
    "            break\n",
    "\n",
    "# Create the pipeline using using the trained modules and save it.\n",
    "accelerator.wait_for_everyone()\n",
    "if accelerator.is_main_process:\n",
    "    controlnet = accelerator.unwrap_model(controlnet)\n",
    "    controlnet.save_pretrained(args.output_dir)\n",
    "\n",
    "    if args.push_to_hub:\n",
    "        save_model_card(\n",
    "            repo_id,\n",
    "            image_logs=image_logs,\n",
    "            base_model=args.pretrained_model_name_or_path,\n",
    "            repo_folder=args.output_dir,\n",
    "        )\n",
    "        upload_folder(\n",
    "            repo_id=repo_id,\n",
    "            folder_path=args.output_dir,\n",
    "            commit_message=\"End of training\",\n",
    "            ignore_patterns=[\"step_*\", \"epoch_*\"],\n",
    "        )\n",
    "\n",
    "accelerator.end_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9511d74-c451-4257-8750-7b51d3d97c95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aceecab0-bc05-4d09-9ff9-8f2cf6a041bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
